{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import matplotlib.pyplot as plt\n",
    "import theano.tensor as tt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of unknown parameters (i.e. dimension of theta in posterior)\n",
    "nparam = 3\n",
    "\n",
    "# Number of draws from the distribution\n",
    "ndraws = 1000\n",
    "\n",
    "# Number of burn-in samples\n",
    "nburn = 500\n",
    "\n",
    "# Metropolis tuning parameters\n",
    "tune = True\n",
    "tune_interval = 100\n",
    "discard_tuning = True\n",
    "\n",
    "# Number of independent chains\n",
    "nchains = 4\n",
    "\n",
    "# Do blocked/compounds sampling in Metropolis and MLDA \n",
    "# Note: This choice applies only to the coarsest level in MLDA \n",
    "# (where a Metropolis sampler is used), all other levels use block sampling\n",
    "blocked = True\n",
    "\n",
    "# Set the sigma for inference\n",
    "sigma = 0.01\n",
    "\n",
    "# Sampling seed\n",
    "sampling_seed = 12345\n",
    "\n",
    "# Guess for prior - assume isotropic, homogeneous linear elasticity\n",
    "\n",
    "E = np.exp(0.0)\n",
    "nu = 0.3\n",
    "\n",
    "G, lamb = E/(2*(1 + nu)), E * nu/((1 + nu)*(1 - 2 * nu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a Theano Op along with the code within ./mlda to construct the likelihood\n",
    "\n",
    "def my_loglik(my_model, theta, datapoints, data, sigma):\n",
    "    output = model_wrapper(my_model, theta, datapoints)\n",
    "    return - (0.5 / sigma ** 2) * np.sum((output - data) ** 2)\n",
    "\n",
    "class LogLike(tt.Op):\n",
    "    itypes = [tt.dvector]  # expects a vector of parameter values when called\n",
    "    otypes = [tt.dscalar]  # outputs a single scalar value (the log likelihood)\n",
    "\n",
    "    def __init__(self, my_model, loglike, data, x, sigma):\n",
    "\n",
    "        # add inputs as class attributes\n",
    "        self.my_model = my_model\n",
    "        self.likelihood = loglike\n",
    "        self.data = data\n",
    "        self.x = x\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        theta = inputs  # this will contain my variables\n",
    "        # call the log-likelihood function\n",
    "        logl = self.likelihood(self.my_model, theta, self.x, self.data, self.sigma)\n",
    "        outputs[0][0] = np.array(logl) # output the log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up finest model and perform inference with PyMC3, using the MLDA algorithm\n",
    "# and passing the coarse_models list created above.\n",
    "\n",
    "traces = []\n",
    "runtimes = []\n",
    "acc = []\n",
    "ess = []\n",
    "\n",
    "\n",
    "with pm.Model():\n",
    "    \n",
    "    # Define priors on parameters\n",
    "    param = [ None ] * 4\n",
    "    \n",
    "    BoundedNormal = pm.Bound(pm.Normal, lower=0.0)\n",
    "    \n",
    "    param[0] = BoundedNormal('C10', mu = 0.5 * G, sigma = 1.0)\n",
    "    param[1] = BoundedNormal('C01', mu = 0.0, sigma = 1.0)\n",
    "    param[2] = BoundedNormal('C11', mu = 0.0, sigma = 1.0)\n",
    "    param[3] = BoundedNormal('D1', mu = 0.5 * lamb, sigma = 1.0)\n",
    "\n",
    "    # Convert m and c to a tensor vector\n",
    "    theta = tt.as_tensor_variable(parameters)\n",
    "\n",
    "    # use a DensityDist (use a lamdba function to \"call\" the Op)\n",
    "    pm.DensityDist('likelihood', lambda v: logl(v), observed={'v': theta})\n",
    "    \n",
    "    map_estimate = pm.find_MAP\n",
    "\n",
    "    # Initialise a demetropolis step method.\n",
    "    step_demetropolis = pm.DEMetropolis(tune='scaling', tune_interval=tune_interval)\n",
    "\n",
    "    # Inference! \n",
    "    # DEMetropolis\n",
    "    t_start = time.time()\n",
    "    method_names.append(\"DEMetropolis\")\n",
    "    traces.append(pm.sample(draws=ndraws, step=step_demetropolis,\n",
    "                            chains=nchains, tune=nburn,\n",
    "                            discard_tuned_samples=discard_tuning,\n",
    "                            random_seed=sampling_seed))\n",
    "    runtimes.append(time.time() - t_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
